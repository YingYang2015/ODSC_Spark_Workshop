{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "If you are using a transformed dataset for multiple operations, persisting the dataset in memory is a great way to improve performance by as much as 10x. Here we walk through some examples of using persistence in Spark. \n",
    "\n",
    "We are going to be using the Reddit comments dataset for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Since the reddit dataset is so large, we'll start with just the 2007 data, which is in a parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"controversiality\", LongType(), True),\n",
    "        StructField(\"created_utc\", StringType(), True),\n",
    "        StructField(\"day\", LongType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"downs\", LongType(), True),\n",
    "        StructField(\"edited\", StringType(), True),\n",
    "        StructField(\"gilded\", LongType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"link_id\", StringType(), True),\n",
    "        StructField(\"month\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), True),\n",
    "        StructField(\"retrieved_on\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"score_hidden\", BooleanType(), True),\n",
    "        StructField(\"subreddit\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), True),\n",
    "        StructField(\"ups\", LongType(), True),\n",
    "        StructField(\"year\", LongType(), True)]\n",
    "#rawDF = sqlContext.read.json(\"s3a://reddit-comments/2008\", StructType(fields))\n",
    "rawDF = sqlContext.read.parquet(\"s3a://reddit-comments-parquet/year=2007\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to convert a timestamp (created_utc field) into a dict with year, month, and day.\n",
    "def cast_ts_to_ymd(row):\n",
    "    row_dict = row.asDict()\n",
    "    utc_dt = datetime.fromtimestamp(int(row_dict['created_utc']))\n",
    "    row_dict['year'] = utc_dt.year\n",
    "    row_dict['month'] = utc_dt.month\n",
    "    row_dict['day'] = utc_dt.day\n",
    "        \n",
    "    return Row(**row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function cast_ts_to_ymd to the dataframe using the .map and .toDF transformations. We will then create a table called 'rc' which we can use for calling SparkSQL commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateDF = rawDF.rdd.map(cast_ts_to_ymd).toDF(StructType(fields))\n",
    "dateDF.registerTempTable(\"rc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence\n",
    "One of the most important capabilities in Spark is persisting a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x).\n",
    "\n",
    "You can mark an RDD to be persisted using the **persist()** method on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.\n",
    "\n",
    "In addition, each persisted RDD can be stored using a different storage level, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes, or store it off-heap in Tachyon. These levels are set by passing a StorageLevel object (Scala, Java, Python) to persist(). \n",
    "\n",
    "The **cache()** method is an alias for **persist()** using the default storage level (StorageLevel.MEMORY_ONLY) which is memory only persisting. Good for quick prototyping work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's persist the dataframe after we have transformed the timestamp (dateDF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateDF.persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute a series of queries that use the new date format. First, let's calculate the number of unique subreddits per author per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_subreddit_author = sqlContext.sql(\"\"\"\n",
    "    SELECT author, month, COUNT(DISTINCT subreddit) as cnt\n",
    "    FROM rc\n",
    "    GROUP BY author, month\n",
    "    ORDER BY month, cnt DESC\n",
    "    \"\"\")\n",
    "unique_subreddit_author.registerTempTable(\"subreddits_per_author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can calculate the average number of unique subreddits per author for each month in 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_subreddit_per_author = sqlContext.sql(\"\"\"\n",
    "    SELECT month, AVG(cnt) as avg_cnt\n",
    "    FROM subreddits_per_author\n",
    "    GROUP BY month\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally run this job using the .toPandas() action and plot the result (may take a few minutes, check your 4040 status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_subreddit_per_author_pDF = average_subreddit_per_author.toPandas()\n",
    "average_subreddit_per_author_pDF.plot(x='month', y='avg_cnt')\n",
    "plt.ylabel('Average Unique Subreddits per Author')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which subreddits have the most contributing authors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_authors_subreddit = sqlContext.sql(\"\"\"\n",
    "    SELECT subreddit, COUNT(DISTINCT author) as cnt\n",
    "    FROM rc\n",
    "    GROUP BY subreddit\n",
    "    ORDER BY cnt DESC\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_authors_subreddit_pDF = unique_authors_subreddit.toPandas()\n",
    "unique_authors_subreddit_pDF[:20].plot(x='subreddit', y='cnt', kind='barh', alpha=0.5, log=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Which author contributed most to the worldnews subreddit in any given month?\n",
    "\n",
    "** Hint: need to avoid edits such as 'deleted' **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: How many authors who commented on the worldnews subreddit also commented on any of the top 20 commented subreddits? \n",
    "#### Hint: to optimize you should use one or more persists... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
