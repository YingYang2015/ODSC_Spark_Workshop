{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row, functions as F\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, Binarizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this tutorial, we will use spark.ml to build an ML pipeline using the reddit comment dataset.\n",
    "\n",
    "We are going to build a model to classify reddit comments as positive or negative based on the content of each comment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Reddit data\n",
    "#### Reading in this dataset takes approximately 17 minutes. A good time for a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"controversiality\", LongType(), True),\n",
    "        StructField(\"created_utc\", StringType(), True),\n",
    "        StructField(\"day\", LongType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"downs\", LongType(), True),\n",
    "        StructField(\"edited\", StringType(), True),\n",
    "        StructField(\"gilded\", LongType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"link_id\", StringType(), True),\n",
    "        StructField(\"month\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), True),\n",
    "        StructField(\"retrieved_on\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"score_hidden\", BooleanType(), True),\n",
    "        StructField(\"subreddit\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), True),\n",
    "        StructField(\"ups\", LongType(), True),\n",
    "        StructField(\"year\", LongType(), True)]\n",
    "# You can try the json read later, but it's mainly to understand the performance benefit between json and parquet file formats\n",
    "#rawDF = sqlContext.read.json(\"s3a://reddit-comments/2009\", StructType(fields)).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rawDF = sqlContext.read.parquet(\"s3a://reddit-comments-parquet/year=2009\").persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a subset of the data\n",
    "To keep the tutorial fast, we'll just use the reddit.com subreddit comments here. You could easily extend this to a general model for comments across all subreddits.\n",
    "\n",
    "We clean the data by removing deleted comments and comments that do not have a score. We also cast the columns into the correct datatype for Spark's Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns that are needed for the training and testing\n",
    "# Cast columns to the correct datatype for Transformers\n",
    "# Only use comments that have been upvoted or downvoted\n",
    "def cast_col(df, col, cast_type):\n",
    "    '''\n",
    "    Function to cast column into datatype for Transformers. \n",
    "    The format may seem very un-pythonic but because Spark is written in Scala, \n",
    "    the columns are immutable and so we need to create a new temporary column (temp_col)\n",
    "    '''\n",
    "    return df.withColumn(\"temp_col\", df[col].cast(cast_type))\\\n",
    "             .drop(col)\\\n",
    "             .withColumnRenamed(\"temp_col\", col)\n",
    "\n",
    "# filter out comments with no score, comments that are deleted, and only use the reddit.com subreddit comments\n",
    "filteredDF = rawDF.select(\"id\", \"body\", \"score\", \"score_hidden\", \"subreddit\")\\\n",
    "                  .filter(rawDF.body != \"[deleted]\")\\\n",
    "                  .filter(rawDF.score_hidden == False)\\\n",
    "                  .filter(rawDF.subreddit == \"reddit.com\")\n",
    "castedDF = cast_col(filteredDF, \"score\", DoubleType())\n",
    "\n",
    "print \"Sample size: {}\".format(castedDF.count())\n",
    "\n",
    "castedDF.registerTempTable(\"rc\")\n",
    "query = sqlContext.sql(\"\"\"\n",
    "    SELECT score, COUNT(*) as cnt FROM rc\n",
    "    GROUP BY score\n",
    "    ORDER BY cnt DESC\n",
    "    \"\"\")\n",
    "result = query.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distribution of comment scores to see what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.plot(x=\"score\", y=\"cnt\", kind=\"scatter\")\n",
    "plt.xlim([-20,20])\n",
    "plt.ylim([0, 800000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most comments have a score of 1, and the number of comments drops very rapidly on either side of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a labeled dataset\n",
    "Now that we have cleaned the data, let's create labels for our positive comments and negative comments. \n",
    "\n",
    "Since the majority of comments have a score of 0-3, in this example we are going to assume that a comment needs a score < 0 to be a negative comment and > 10 to be a positive comment (10% of the data falls into this classification scheme). \n",
    "\n",
    "You probably noticed that conveniently the number of positive comments and the number of negative comments are about equal. A balanced labeled dataset will help simplify training and validating our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negativeDF = castedDF.filter(castedDF[\"score\"] < 0)\n",
    "positiveDF = castedDF.filter(castedDF[\"score\"] > 10)\n",
    "\n",
    "print negativeDF.count(), positiveDF.count(), castedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing data\n",
    "Now let's combine the positive comments and negative comments, and randomly split them into training and testing datasets (80% in the training set, 20% in the testing set). \n",
    "\n",
    "We'll put the testing dataset aside for now and use the training dataset to train our model. Once the model is trained, we can use the testing dataset to validate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and testing\n",
    "mergedDF = negativeDF.unionAll(positiveDF)\n",
    "splitDF = mergedDF.randomSplit([0.8, 0.2])\n",
    "trainingDF = splitDF[0]\n",
    "testingDF = splitDF[1]\n",
    "\n",
    "trainingDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "testingDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "print \"training size: {}\".format(trainingDF.count())\n",
    "print \"negative sentiment: {}\".format(trainingDF.filter(trainingDF.score<0).count())\n",
    "print \"positive sentiment: {}\".format(trainingDF.filter(trainingDF.score>0).count())\n",
    "print \"testing size: {}\".format(testingDF.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "As features for our model we are going to use the frequency of each word in a comment.\n",
    "\n",
    "We build an ML pipeline by chaining together the binarizer, tokenizer, hashingTF and logisticregression. Then we  fit the model to the training dataset and make predictions on the testing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#binarize -- create a column called 'label' that has converted the score into a column \n",
    "#that contains a 0 or 1, depending on the threshold variable\n",
    "binarizer = Binarizer(threshold=0.0, inputCol=\"score\", outputCol=\"label\")\n",
    "#tokenize the text into individual words\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "#hasgingTF - calculate the term frequency and send the resulting values to a column called 'features'\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "#now create a logistic regression\n",
    "#maxIter is the maximum number of iterations completed when running the fit\n",
    "#regParam is the regularization strength\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "#put it all into a pipeline estimator, fit on the training data, and test on the testing data!\n",
    "pipeline = Pipeline(stages=[binarizer, tokenizer, hashingTF, lr])\n",
    "model = pipeline.fit(trainingDF)\n",
    "prediction = model.transform(testingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "### Let's check how well we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator() # the default metric is the area under the RoC curve\n",
    "train_RoC = evaluator.evaluate(model.transform(trainingDF))\n",
    "test_RoC = evaluator.evaluate(model.transform(testingDF))\n",
    "\n",
    "print \"The area under the RoC curve for the training set is {} and for the test set is {}\".format(train_RoC, test_RoC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's also explore the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected = prediction.select(\"id\", \"body\", \"prediction\", \"label\")\n",
    "positive_score_rate = binarizer.transform(mergedDF).map(lambda r: r.label).mean()\n",
    "\n",
    "def typeI_II(row):\n",
    "    if row.prediction == 0 and row.label == 0:\n",
    "        return Row(error_type=\"true_neg\", cnt=1)\n",
    "    elif row.prediction == 0 and row.label == 1:\n",
    "        return Row(error_type=\"false_neg\", cnt=1)\n",
    "    elif row.prediction == 1 and row.label == 0:\n",
    "        return Row(error_type=\"false_pos\", cnt=1)\n",
    "    else:\n",
    "        return Row(error_type=\"true_pos\", cnt=1)\n",
    "\n",
    "typeI_II_DF = selected.map(lambda r: typeI_II(r)).toDF()\n",
    "type_error_pd = typeI_II_DF.groupBy(\"error_type\")\\\n",
    "                           .sum(\"cnt\")\\\n",
    "                           .withColumnRenamed(\"SUM(cnt)\", \"cnt\").toPandas()\n",
    "\n",
    "type_error_pd[\"tot\"] = type_error_pd[\"cnt\"].sum(axis=0)\n",
    "type_error_pd[\"perc\"] = type_error_pd[\"cnt\"]/type_error_pd[\"tot\"]\n",
    "print type_error_pd\n",
    "print \"percentage of comments with positive score in full set: {0:.2f}\".format(positive_score_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Based on the training and test area under RoC, we've really overfit! Consider why. Which other models or analysis steps do we need?\n",
    "Check into the elasticNetParam option to determine what type of regularlization we have used in our default model. \n",
    "Additionally, one could consider trying a Naive Bayes model, or using the pyspark.ml.feature.IDF transformer, or removing stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could also tune the hyperparameters using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a grid of hyperparameters - careful, this could take a very long time to run with many points in the grid!\n",
    "lr_grid = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[binarizer, tokenizer, hashingTF, lr_grid])\n",
    "\n",
    "grid = ParamGridBuilder()\\\n",
    "        .baseOn({lr_grid.labelCol: 'label'})\\\n",
    "        .addGrid(lr_grid.regParam, [0.01, 0.1])\\\n",
    "        .addGrid(lr_grid.elasticNetParam, [0.5,0.75])\\\n",
    "        .addGrid(lr_grid.maxIter, [1, 2])\\\n",
    "        .build()\n",
    "    \n",
    "print 'optimized over a grid of {} parameter combinations'.format(len(grid))\n",
    "evaluator = BinaryClassificationEvaluator() # the default is the area under the RoC curve\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator, numFolds=4)\n",
    "cvModel = cv.fit(trainingDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluator.evaluate(cvModel.transform(trainingDF))\n",
    "print evaluator.evaluate(cvModel.transform(testingDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We've reduced the overfitting and also have a simple model that performs a bit better than random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
