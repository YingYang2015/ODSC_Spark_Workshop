{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Spark will usually split your data into a set of partitions automatically but there are cases where you want to do this manually to improve performance. You can us the repartition() method to define the number of partitions in your RDD. In this tutorial we explore the number of partitions on performance.\n",
    "\n",
    "We are going to be using the Reddit comments dataset for this tutorial. More information on this dataset can be found [here](https://sites.google.com/a/insightdatascience.com/spark-lab/s3-data/reddit-comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"controversiality\", LongType(), True),\n",
    "        StructField(\"created_utc\", StringType(), True),\n",
    "        StructField(\"day\", LongType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"downs\", LongType(), True),\n",
    "        StructField(\"edited\", StringType(), True),\n",
    "        StructField(\"gilded\", LongType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"link_id\", StringType(), True),\n",
    "        StructField(\"month\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), True),\n",
    "        StructField(\"retrieved_on\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"score_hidden\", BooleanType(), True),\n",
    "        StructField(\"subreddit\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), True),\n",
    "        StructField(\"ups\", LongType(), True),\n",
    "        StructField(\"year\", LongType(), True)]\n",
    "#rawDF = sqlContext.read.json(\"s3a://reddit-comments/2008\", StructType(fields))\n",
    "rawDF = sqlContext.read.parquet(\"s3a://reddit-comments-parquet/year=2008\")\n",
    "rawDF.persist(StorageLevel.DISK_ONLY)\n",
    "rawDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism - example 1\n",
    "\n",
    "To analyze the performance of a simple Spark job with different numbers of partitions, let's split our data into varying numbers of partitions. We'll persist the partitions to save recomputing down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up DataFrames to have various number of partitions ranging from 2 to 32\n",
    "# All will be persisted on disk and the same job will be run on each one to show \n",
    "# performance benefits of different number of partitions\n",
    "repart_1000_df = rawDF.repartition(1000).persist(StorageLevel.DISK_ONLY)\n",
    "repart_2000_df = rawDF.repartition(2000).persist(StorageLevel.DISK_ONLY)\n",
    "repart_4000_df = rawDF.repartition(4000).persist(StorageLevel.DISK_ONLY)\n",
    "repart_8000_df = rawDF.repartition(8000).persist(StorageLevel.DISK_ONLY)\n",
    "repart_16000_df = rawDF.repartition(16000).persist(StorageLevel.DISK_ONLY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform a simple job (count the number of elements in each dataframe) and compare how long the operation takes for various number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run through each DataFrame and count the number of elements in each. This will \n",
    "# trigger the DataFrames to persist into Memory and Disk\n",
    "df_arr = [(rawDF, rawDF.rdd.getNumPartitions()),\n",
    "          (repart_1000_df, repart_1000_df.rdd.getNumPartitions()),\n",
    "          (repart_2000_df, repart_2000_df.rdd.getNumPartitions()),\n",
    "          (repart_4000_df, repart_4000_df.rdd.getNumPartitions()),\n",
    "          (repart_8000_df, repart_8000_df.rdd.getNumPartitions()),\n",
    "          (repart_16000_df, repart_16000_df.rdd.getNumPartitions())]\n",
    "for df in df_arr:\n",
    "    start_time = time()\n",
    "    df[0].count()\n",
    "    end_time = time()\n",
    "    print \"{} partitions took {} seconds to repartition and count\".format(df[1], end_time - start_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the runtime indicate about the scheduling and distribution of tasks to worker nodes?\n",
    "\n",
    "### Look at 4040 to see how the count action varied in time with increased number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism - example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function sorts an array of UTC time and calculates the delta time in days between each consecutive UTC time\n",
    "# Returns a Row object containing the subreddit, the median time delta representing the median time in days \n",
    "# between comments for any subreddit, and the total number of comments in each subreddit\n",
    "def calc_median(row):\n",
    "    from heapq import heappop\n",
    "    from numpy import median\n",
    "    \n",
    "    subreddit = row[0]\n",
    "    val_arr = row[1]\n",
    "    num_comments = len(val_arr)\n",
    "    \n",
    "    dt = []\n",
    "\n",
    "    if len(val_arr) > 1:\n",
    "        prev_val = heappop(val_arr)\n",
    "        while len(val_arr) > 0:\n",
    "            curr_val = heappop(val_arr)\n",
    "            dt.append(curr_val - prev_val)\n",
    "            prev_val = curr_val\n",
    "        return Row(subreddit=subreddit, median_time_days=float(median(dt))/60.0/60.0/24.0, num_comments=num_comments)\n",
    "    else:\n",
    "        return Row(subreddit=subreddit, median_time_days=0.0, num_comments=num_comments)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is very inefficient. Can you figure out why (before attempting to run it)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through each DataFrame and run the same job to calculate the median time between comments for each\n",
    "# subreddit\n",
    "def combiner(value):\n",
    "    return value\n",
    "\n",
    "def merger(x, value):\n",
    "    from heapq import heappush\n",
    "\n",
    "    heappush(x, value[0])\n",
    "\n",
    "    return x\n",
    "\n",
    "def merge_combiner(x, y):\n",
    "    from heapq import heappush, heappop\n",
    "\n",
    "    while len(y) > 0:\n",
    "        heappush(x, heappop(y))\n",
    "\n",
    "    return x\n",
    "\n",
    "results = []\n",
    "for df in df_arr:\n",
    "    start_time = time()\n",
    "    curr_df = df[0]\n",
    "    num_partitions = df[1]\n",
    "\n",
    "    subreddit_comment_times = curr_df.rdd.map(lambda r: (r.subreddit, [int(r.created_utc)]))\n",
    "    median_time_between_posts_df = subreddit_comment_times.combineByKey(combiner, merger, merge_combiner)\\\n",
    "                                                          .map(calc_median)\\\n",
    "                                                          .toDF()\n",
    "\n",
    "    total_cnt = median_time_between_posts_df.count()\n",
    "    end_time = time()\n",
    "    print \"{} partitions took {} seconds to process, {} final records\".format(df[1], end_time - start_time, total_cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A scatter plot where each element is a subreddit. We can see that subreddits with more than 40 comments tend to\n",
    "# also have more frequent comment activity throughout the year\n",
    "median_time_between_posts_pd = median_time_between_posts_df.toPandas()\n",
    "median_time_between_posts_pd.plot(kind='scatter', x='num_comments', y='median_time_days', xlim=(-5, 100), ylim=(-5, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: What is the optimal number of partitions for calculating the author who has written the longest comment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
