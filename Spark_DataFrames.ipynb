{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Spark has the conceptual equivalent of R and pandas dataframes available through the DataFrames API. In Spark DataFrames may be created from existing RDDs, structured data files, external databases, or tables in Hive, etc. These DataFrames allow for the manipulation of distributed data through operations such as filter, groupby, and join, which are automatically optimized through parallelization and distribution across clusters when an action triggers the job.\n",
    "\n",
    "We are going to use the dating profiles dataset for this tutorial. More information on this dataset can be found [here](https://sites.google.com/a/insightdatascience.com/spark-lab/s3-data/dating-profiles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the data\n",
    "First, read in the data into an RDD. Here, we are using 16 partitions for our RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in raw ratings data (fromUserId, toUserId, rating)\n",
    "ratingsCsvRDD = sc.textFile(\"s3a://insight-spark-after-dark/ratings.csv.gz\").repartition(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data into a JSON format by applying the function rec_tup (which converts the data into a JSON format) to the RDD using the .map and .toDF() transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert raw ratings RDD to Json RDD\n",
    "def rec_tup(rating):\n",
    "    tokens = rating.split(\",\")\n",
    "    return Row(fromUserId=int(tokens[0]), toUserId=int(tokens[1]), rating=int(tokens[2]))\n",
    "\n",
    "ratingsJson_DF = ratingsCsvRDD.map(rec_tup).toDF()\n",
    "ratingsJson_DF.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might remember, Spark does a Lazy evaluation which means each transformed RDD may be recomputed each time you run an action on it. We'll use the .persist method (this is actually also a transformation) to keep the result (transformed RDD) on the cluster for quick future access.\n",
    "\n",
    "**Note**: persist is a transformation, it will only run when a future action is called. One result of this is that if you run an action twice, you will persist twice (and right now in Spark you will lose the pointer to the first persist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cache the SchemaRDD as we'll be using this heavily moving forward\n",
    "ratingsJson_DF.persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly check the schema of the dataframe. \n",
    "To write SparkSQL, we need to create a table object from our dataframe which we can use to run SparkSQL commands. The transformation registerTempTable() does this and we call our table 'ratingsJsonTable'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the SchemaRDD inferred from the JSON\n",
    "ratingsJson_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On to Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrames is an abstract API that lets you perform Pandas or R like commands to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the top 10 most-active users who are giving out ratings\n",
    "mostActiveUsersSchemaDF = ratingsJson_DF.select(\"fromUserId\",\"toUserId\")\\\n",
    "                                        .groupBy(\"fromUserId\")\\\n",
    "                                        .count()\\\n",
    "                                        .withColumnRenamed(\"count\", \"num_ratings\")\\\n",
    "                                        .orderBy([\"num_ratings\"], ascending=[0])\n",
    "mostActiveUsersSchemaDF.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "We can also sample the dataframe with the sample() transformation. Great way to test the job before running on the full dataset.\n",
    "\n",
    "Sample takes 3 arguments: **oversample (True/False), fraction of data in sample, seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingsJson_DF.sample(False, 0.1, 20).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some further questions to get to grips with Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Who are the top 5 users that gives the highest average rating of other profiles? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: How many pairs of users have rated each other with a rating > 5? (Is not intuitive...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
